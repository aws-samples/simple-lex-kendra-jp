# 生成系 AI と Amazon Kendra

生成系 AI は革新的な技術であり、ビジネスに変革をもたらす可能性があります。Amazon Kendra と組み合わせることにより、効果的な情報アクセスを実現できます。

Amazon Kendra を単体で利用する場合、ユーザーは Amazon Kendra が検索したドキュメントを自身で読んで理解する必要があります。一方、生成系 AI と組み合わせて利用することで、ユーザーの要求に合わせてドキュメントを要約したり、よりわかりやすい文章に置き換えたりして表示することが可能になります。また、Amazon Kendra から検索したドキュメントをもとに、生成系 AI がユーザーの質問に答えることもできます。

## RAG とは

RAG ( Retrieval Augmented Generation )は、情報の検索と文章の生成 (ここでは LLM ( 大規模言語モデル ) を指す ) を組み合わせる手法のことです。Amazon Kendra で関連するドキュメントを検索し、その検索したドキュメントを生成系 AI にコンテキストとして与えることで、Fine-Tuning 無しで社内ドキュメントに対応した文章生成を行うことができます。  
ただし、LLM の入力プロンプトには文字数制限（トークン数の制限）があるため、検索したドキュメントをそのまま設定することができないことが多いでしょう。そのため、ドキュメント内の関連する部分だけを抜粋して、LLM に入力する必要があります。RAG の文章生成の精度を上げるためには、Amazon Kendra のような関連ドキュメントを検索し、かつ関連する部分だけを抜粋してくれる高性能な検索サービスが必要不可欠となります。

## RAG を使うメリット

RAG を使うメリットは、以下の通りです。

- 事前学習していないデータに対応できる
  - 最新のデータや社内ドキュメントなどの、学習していないデータに対しても回答できるようになる
    - 実施のハードルが高い Fine-Tuning をすることなく、これらに対応することが可能
- カスタマイズの容易性
  - Prompt Engineering と呼ばれる入力プロンプトの調整作業で、さまざまなタスクに対応可能
- 大規模データへの対応
  - Amazon Kendra を利用すれば、数百万件のドキュメントから検索することが可能であるため、大量の社内ドキュメントに対応できる LLM の処理を構築することが可能

## 本 RAG アセットの注意点

こちらのアセットは、**Amazon Bedrock の Claude Instant (基盤モデル) に最適化して開発しています。** Claude 以外の基盤モデルについては、動作や精度を担保しておりませんので、ご注意ください。

詳しくは、[こちらのデプロイ手順](./03_DEPLOY_KENDRA.md#amazon-bedrock-の事前設定)をご確認ください。

## 処理の流れ

このアセットの RAG によるチャット形式のドキュメント検索は、以下の流れで処理を行っています。

![rag-flow](/imgs/rag-flow.drawio.png)

### 処理の補足解説

- Retrieve 用 Query 生成のプロンプトを実行
  - RAG で非常に重要となる情報の検索は、Amazon Kendra の [Retrieve API](https://docs.aws.amazon.com/ja_jp/kendra/latest/APIReference/API_Retrieve.html) を利用しますが、以下の理由からこの API で利用する Query を LLM で生成します。
    - Retrieve API の Query が 30 トークンを超えると、自動で切り捨てられて検索されるため。
    - ユーザからの質問文をそのまま Query にするよりも、Query を再編成した方が検索精度が良くなるため。
      - 以下、Query 再編成の例
        - 「Kendra について教えて」という質問の場合は、ユーザは概要を知りたがっているので「Kendraの概要」と再編成する。
        - 「Kendraのメタデータについて教えて」「要約して」「登録方法を教えて」という質問が続いている場合、直近の質問でユーザが聞きたいことは「メタデータの登録方法」となる。「要約して」というような指示はすべて無視し、主語を補完し「Kendraのメタデータの登録方法」と再編成する。
- 質問プロンプトを実行
  - 取得した関連ドキュメントをコンテキストとして設定して、ユーザの質問の回答を LLM で生成します。
  - プロンプトで回答の手順と回答のルールを定義し、それに沿った回答を生成するように指示しています。
    - たとえば、「参考ドキュメントにない情報は回答しない」「雑談には応答しない」などのルールを定義しています。
- 参考ドキュメント取得のプロンプトを実行
  - 質問のコンテキストとして設定した関連ドキュメントの中で、どのドキュメントをもとに回答したのかを JSON 形式で出力し、参考情報として画面に表示することで、ユーザは回答のソースに簡単にアクセスすることが可能になります。
